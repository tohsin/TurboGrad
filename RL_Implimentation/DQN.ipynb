{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing first games\n",
      "score intial 11.5\n",
      "loss 275.46449737202437\n",
      "SCORE 12.8\n",
      "loss 264.1348882009976\n",
      "SCORE 11.3\n",
      "loss 248.0997071590833\n",
      "SCORE 11.4\n",
      "loss 228.4080534747118\n",
      "SCORE 11.9\n",
      "loss 198.78827610785655\n",
      "SCORE 10.8\n",
      "loss 171.43454252908913\n",
      "SCORE 12.5\n",
      "loss 139.97896048374352\n",
      "SCORE 11.2\n",
      "loss 107.65920452845218\n",
      "SCORE 11.5\n",
      "loss 81.60878221058795\n",
      "SCORE 12.4\n",
      "loss 55.72016699590076\n",
      "SCORE 12.3\n",
      "loss 46.71678281857827\n",
      "SCORE 11.8\n",
      "loss 40.40953317245008\n",
      "SCORE 13.1\n",
      "loss 38.91289547796125\n",
      "SCORE 11.8\n",
      "loss 41.37886532618235\n",
      "SCORE 12.9\n",
      "loss 51.37340398404963\n",
      "SCORE 13.4\n",
      "loss 57.08720833606379\n",
      "SCORE 12.5\n",
      "loss 73.12610313594021\n",
      "SCORE 12.2\n",
      "loss 85.47029588974387\n",
      "SCORE 11.6\n",
      "loss 91.30512511200605\n",
      "SCORE 13.0\n",
      "loss 106.14474879833728\n",
      "SCORE 11.8\n",
      "loss 104.15883904627793\n",
      "SCORE 12.1\n",
      "loss 113.78818158719882\n",
      "SCORE 11.2\n",
      "loss 110.66074262298493\n",
      "SCORE 10.6\n",
      "loss 108.6614103561748\n",
      "SCORE 10.4\n",
      "loss 105.16508107878515\n",
      "SCORE 9.4\n",
      "loss 87.72691427001728\n",
      "SCORE 9.6\n",
      "loss 88.33613076752343\n",
      "SCORE 9.2\n",
      "loss 81.14396952572987\n",
      "SCORE 9.7\n",
      "loss 72.05307510676049\n",
      "SCORE 9.3\n",
      "loss 77.51705352953587\n",
      "SCORE 9.6\n",
      "loss 64.77120654740288\n",
      "SCORE 9.4\n",
      "loss 64.31051168596301\n",
      "SCORE 9.4\n",
      "loss 63.03908492133723\n",
      "SCORE 9.4\n",
      "loss 74.27101661853537\n",
      "SCORE 9.4\n",
      "loss 76.77716638723314\n",
      "SCORE 9.4\n",
      "loss 72.37956206170959\n"
     ]
    }
   ],
   "source": [
    "from turbograd.engine_ import Value\n",
    "from turbograd.nn import Layer,  Neuron, Module\n",
    "from turbograd.functional import detach, MSE\n",
    "from turbograd.optim import Optimiser\n",
    "import random\n",
    "import gym\n",
    "\n",
    "class Agent(Module):\n",
    "    def __init__(self, nin, nouts) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1],'relu') for i in range(len(nouts)-1)]\n",
    "        self.layers.append(Layer(sz[-2], sz[-1], ''))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "def play():\n",
    "    scores = []\n",
    "    h = 50\n",
    "    iter = 0\n",
    "    for _ in range(10):\n",
    "        done = False     \n",
    "        score = 0  \n",
    "        obs = env.reset()\n",
    "        obs = obs[0].tolist()\n",
    "        iter = 0\n",
    "        while not done or iter< h:\n",
    "            q_values = actor(obs)\n",
    "            if q_values[0].data> q_values[1].data:\n",
    "                    action = 0\n",
    "            else:\n",
    "                    action = 1\n",
    "            new_obs, reward, done,_, info = env.step(action)\n",
    "            score += reward\n",
    "            obs = new_obs.tolist()\n",
    "            iter+=1\n",
    "        scores.append(score)\n",
    "    score_avg = sum(scores)/ len(scores)\n",
    "    return score_avg\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "action_space = env.action_space\n",
    "observation_space = env.action_space\n",
    "epsilon = 0.4\n",
    "actor = Agent(4, [32, 32, action_space.n])\n",
    "epochs = 150\n",
    "batch_size = 200\n",
    "from collections import deque\n",
    "losses = []\n",
    "avg_scores = []\n",
    "buffer = deque(maxlen=1000)\n",
    "h = 50 # max number of moves\n",
    "gamma = 0.99\n",
    "optimiser = Optimiser('SGD', actor)\n",
    "print(\"playing first games\")\n",
    "avg_score = play()\n",
    "print(\"score intial\", avg_score)\n",
    "avg_scores.append(avg_score)\n",
    "for i in range(epochs):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    obs = obs[0].tolist()\n",
    "    iter = 0\n",
    "    while not done or iter< h:\n",
    "        q_values = actor(obs)\n",
    "        \n",
    "        # action selection in epsilon greedy\n",
    "        pr = random.random()\n",
    "        if (pr < epsilon):\n",
    "            action = random.randint(0,1) \n",
    "        else:\n",
    "            if q_values[0].data> q_values[1].data:\n",
    "                action = 0\n",
    "            else:\n",
    "                action = 1\n",
    "       \n",
    "        new_obs, reward, done,_, info = env.step(action)\n",
    "        new_obs = new_obs.tolist()\n",
    "        mdp = (obs, action, reward, new_obs, done)\n",
    "        buffer.append(mdp)\n",
    "        obs = new_obs\n",
    "        iter+=1\n",
    "\n",
    "        if len(buffer)> batch_size:\n",
    "            minibatch = random.sample(buffer, batch_size)\n",
    "            s_batch = [s1 for (s1, a, r, s2, d ) in minibatch]\n",
    "            a_batch = [a for (s1, a, r, s2, d ) in minibatch]\n",
    "            r_batch = [r for (s1, a, r, s2, d ) in minibatch]\n",
    "            s2_batch = [s2 for (s1, a, r, s2, d ) in minibatch]\n",
    "            d_batch = [d for (s1, a, r, s2, d ) in minibatch]\n",
    "\n",
    "            Q_s = []\n",
    "            Q_s_1 = []\n",
    "            target_buff = []\n",
    "            for i in range(len(s_batch)):\n",
    "                Q_s.append(actor(s_batch[i])[a_batch[i]])\n",
    "            ##  you need to not have the Q values with diffrentiable parameters\n",
    "            for state2 in s2_batch:\n",
    "                Q_s_1.append(detach(actor(state2)))\n",
    "        \n",
    "            target = [reward  +  (gamma * ((1 - d)* max(Q_2))) for reward, d, Q_2 in zip(r_batch, d_batch, Q_s_1 )]\n",
    "            loss : Value = MSE(Q_s , target)\n",
    "          \n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step(lr = 1e-5)\n",
    "            losses.append(loss.data)\n",
    "            print(\"loss\", loss.data)\n",
    "            avg_score = play()\n",
    "            print(\"SCORE\", avg_score)\n",
    "            avg_scores.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(losses))\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mplot(epochs, losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining Loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, len(losses))\n",
    "plt.plot(epochs, losses, label='Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (v3.9.12:b28265d7e6, Mar 23 2022, 18:17:11) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f7eb34cf01def54aa1c1e29a6d82a4f7d72bbd8ee261a0fdeebcc0fa2c33cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
