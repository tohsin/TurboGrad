{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Decent algorithm\n",
    "v = v - alpha * dC/dv\n",
    "\n",
    "Convex graph -> bottom\n",
    "update \n",
    "dC/dv - gradient\n",
    "\n",
    "v -= alpha * grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reason for wprking well\n",
    "Linear and none linear functions\n",
    "\n",
    "# linear regression\n",
    "y = wx + c = intercept\n",
    "\n",
    "1 varaible\n",
    "\n",
    "# logistic regression\n",
    "\n",
    "## linear equaion\n",
    "y = w1 * x1 + w2 * x2 ..... + b\n",
    "\n",
    "w = weights\n",
    "x = input\n",
    "b = bias\n",
    "\n",
    "# Activations\n",
    "sigmoid = 1/ 1 + exp(-z) = binary classification 0, 1,\n",
    "tanh e (-1, 1)\n",
    "[link to activations](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "leaky relu => Assignment\n",
    "relu = max(0, z)\n",
    "None = y => real number\n",
    "sigmoid - Assignment\n",
    "E relu\n",
    "why relu works better as an activation function than sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = w1 * x1 + w2 * x2 +w3 * x3  + b\n",
    "\n",
    "a = 1 / 1 + exp(-z)\n",
    "# supervised learning\n",
    "a - from network- prediction\n",
    "y - label\n",
    "\n",
    "MSE = C = | y - a | ^2\n",
    "|label - pred |^2\n",
    "C is for all\n",
    "\n",
    "L = Ei-n ( y- a)^2 \n",
    "loss is for one\n",
    "\n",
    "# binary classification\n",
    "Logistic cross entropy = - (1 - y * ln (1- a) +   y * ln a )\n",
    "Assignment - intuition behind\n",
    "male = 1\n",
    "female = 0\n",
    "label\n",
    "y = 1 = male\n",
    "\n",
    "prediction = a = 0.1 = female\n",
    "\n",
    "L = 1-1 * log(1-0.1) + 1 * log 0.1\n",
    "L  = 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z = w1 * x1 + w2 * x2 + b \\\\\n",
    "a = 1 / 1 + e(-z) = $$\\sigma(-z)$$\n",
    "L = MSE\n",
    "L = - (1-y) * log(1 - a) + y *log(a)\n",
    "grad = dL/dw1\n",
    "w1 = w1 - alpha *dL/dw1\n",
    "w2 = w2 - alpha *dL/dw2\n",
    "b = b - alpha * dL/db $\n",
    "repeat till convergence\n",
    "\n",
    "\n",
    "# static vs dynamic\n",
    "pytorch dynamic\n",
    "static\n",
    "\n",
    "dl/db = dl/da * da/dz * dz/db\n",
    "dl/db = dl/da * da/dz * dz/db\n",
    "dL/da =>\n",
    "b * ln(1-a)  = (1-y) *  1 * 1/ (1-a) -  y/a\n",
    "= (1 - y)/(1 - a) - y / a\n",
    "\n",
    "da /dz = (1 + e(-z)) ^ -1 = -1 * (1 + e(-z)) ^ -2 * e(-z)\n",
    "= -e(-z)/ (1 + e(-z))^2 = - e(-z)  / (1+ e(-z)) * (1+ e(-z))\n",
    "- e(-z)/ (1+ e(-z)) * (1/ 1+ e(-z))\n",
    "= -s * e(-z) / (1 + e(-z))\n",
    "-s * e(-z) / e(-z) / (1+e(-z))/(e-z)\n",
    "= -s * 1 / 1+e(-z)) / (e-z)\n",
    "-S * 1 / 1/ e(-z) + 1\n",
    "\n",
    "\n",
    "look up diffrentila of sigmoid poroof\n",
    "da / dz = s * (1 -s)\n",
    "\n",
    "dz * db = 1\n",
    "\n",
    "\n",
    "dL/db = (1-y/ 1-a ) - y/a * (a * (1-a))\n",
    "\n",
    "1-y / 1-a - y/a = a(1-y) - (1-a)y / (1-a)a   * a(1-a)\n",
    "a(1-y) - (1-a)y \n",
    "a - ay - y + ay\n",
    "dl/db = a-y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.24026384943841\n",
      "grad of b 0.9997361853670718\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "h = 0.01\n",
    "\n",
    "w1 = 2.0\n",
    "w2 = 1.3\n",
    "b = 0.34\n",
    "\n",
    "x1 = 2\n",
    "x2 = 3\n",
    "\n",
    "z = w1 * x1 + w2 * x2 + b\n",
    "a = 1/ (1 + math.exp(-z))\n",
    "\n",
    "y = 0\n",
    "loss = -(((1-y) * math.log(1-a)) + (y * math.log(a)))\n",
    "print(loss)\n",
    "print(\"grad of b\", (a-y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9997374997041675\n"
     ]
    }
   ],
   "source": [
    "b+= h\n",
    "z_b = w1 * x1 + w2 * x2 + b\n",
    "a_b = 1/ (1 + math.exp(-z_b))\n",
    "\n",
    "y = 0\n",
    "loss_h = -(((1-y) * math.log(1-a_b)) + (y * math.log(a_b)))\n",
    "\n",
    "grad = (loss_h - loss) / h\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (v3.9.12:b28265d7e6, Mar 23 2022, 18:17:11) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f7eb34cf01def54aa1c1e29a6d82a4f7d72bbd8ee261a0fdeebcc0fa2c33cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
